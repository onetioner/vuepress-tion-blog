---
title: Flume
date: 2025-01-17 15:28:08
permalink: /pages/ceb8cc/
categories:
  - 《Flume》笔记
tags:
  - 
author: 
  name: onetion
  link: https://github.com/onetioner
---
# Flume概述

Flume是的一个分布式、高可用、高可靠的海量日志采集、聚合和传输的系统，支持在日志系统中定制各类数据发送方，用于收集数据，同时提供了对数据进行简单处理并写到各种数据接收方的能力。

> 分布式：部署多个Agent来做分布式
>
> 高可用：通过Sink的负载均衡（或者说是副本机制）来保证高可用
>
> 高可靠：是通过Channel事务来保证高可靠
>
> 定制各类数据：指定各种Source
>
> 简单处理：通过拦截器实现修改、丢弃
>
> 各种接收方：指定各种Sink

Flume的设计原理是基于数据流的，能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中心化数据存储系统中。 Flume能够做到近似实时的推送，并且可以满足数据量是持续且量级很大的情况。比如它可以收集社交网站日志，并将这些数量庞大的日志数据从网站服务器上汇集起来，存储到HDFS或 HBase分布式数据库中。

Flume的应用场景:比如一个电商网站，想从网站访问者中访问一些特定的节点区域来分析消费者的购物意图和行为。为了实现这一点，需要收集到消费者访问的页面以及点击的产品等日志信息，并移交到大数据 Hadoop平台上去分析，可以利用 Flume做到这一点。现在流行的内容推送，比如广告定点投放以及新闻私人定制也是基于这个道理。

# Flume架构

![image-20200327112206283](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706044.png)

## Event

事件是Flume内部数据传输的最基本单元，将传输的数据进行封装。事件本身是由一个载有数据的字节数组和可选的headers头部信息构成，如下图所示。Flume以事件的形式将数据从源头传输到最终的目的地。

![image-20200323103129436](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706046.png)

## Agent

Flume Agent 是一个JVM进程，通过三个组件（source、channel、sink）将事件流从一个外部数据源收集并发送给下一个目的地。

### Source

从数据发生器接收数据，并将数据以Flume的Event格式传递给一个或多个通道（Channel）

支持Source:

* [Avro Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#avro-source)
* [Thrift Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#thrift-source)
* [Exec Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#exec-source)
* [JMS Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#jms-source)
* [Spooling Directory Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#spooling-directory-source)
* [Taildir Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#taildir-source)
* [Twitter 1% firehose Source (experimental)](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#twitter-1-firehose-source-experimental)
* [Kafka Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kafka-source)
* [NetCat TCP Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#netcat-tcp-source)
* [NetCat UDP Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#netcat-udp-source)
* [Sequence Generator Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#sequence-generator-source)
* [Syslog Sources](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#syslog-sources)
* [HTTP Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#http-source)
* [Stress Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#stress-source)
* [Legacy Sources](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#legacy-sources)
* [Custom Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#custom-source)

### Channel

一种短暂的存储容器，位于 Source和Sink之间，起着桥梁的作用。 Channel将从Source处接收到的 Event格式的数据缓存起来，当Sink成功地将 Events发送到下一跳的Channel或最终目的地后， Events从 Channel移除。Channel是一个完整的事务，这一点保证了数据在收发的时候的一致性。可以把 Channel看成一个FIFO（先进先出）队列，当数据的获取速率超过流出速率时，将Event保存到队列中，再从队中一个个出来。

有以下几种Channel：

* **Memory Channel** 事件存储在可配置容量的内存队列中，队列容量即为可存储最大事件数量，适用于高吞吐量场景，在agent出现错误时有可能会丢失部分数据
* **File Channel** 基于文件系统的持久化存储
* Spillable Memory Channel 内存和文件混合Channel，当内存队列满了之后，新的事件会存储在文件系统，目前处于实验阶段，不建议在生产环境中使用
* JDBC Channe 事件存储在持久化的数据库中，目前只支持Derby
* Kafka Channel 事件存储在Kafka集群中
* Pseudo Transaction Channel 伪事务Channel，仅用于测试，不能在生产环境使用
* Custom Channel 自定义Channel

### Sink

获取Channel暂时保存的数据并进行处理。sink从channel中移除事件，并将其发送到下一个agent（简称下一跳）或者事件的最终目的地，比如HDFS。

Sink分类：

* [HDFS Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hdfs-sink)
* [Hive Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hive-sink)
* [Logger Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#logger-sink)
* [Avro Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#avro-sink)
* [Thrift Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#thrift-sink)
* [IRC Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#irc-sink)
* [File Roll Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#file-roll-sink) 将Events保存在本地文件系统
* [Null Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#null-sink) 抛弃从Channel接收的所有事件
* [HBaseSinks](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hbasesinks)
* [MorphlineSolrSink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#morphlinesolrsink)
* [ElasticSearchSink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#elasticsearchsink)
* [Kite Dataset Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kite-dataset-sink)
* [Kafka Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kafka-sink)
* [HTTP Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#http-sink)
* [Custom Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#custom-sink)

## 数据流模型

![Agent component diagram](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706047.png)

过程简要说明如下:
（1）外部数据源（Web Server）将Flume可识别的 Event发送到 Source
（2） Source收到 Event事件后存储到一个或多个Channel通道中。
（3）Channel保留 Event直到Sink将其处理完毕。
（4）Sink从 Channel中取出数据，并将其传输至外部存储（HDFS）。



### 可靠性

事件在每个agent的channel中短暂存储，然后事件被发送到下一个agent或者最终目的地。事件只有在存储在下一个channel或者最终存储后才从当前的channel中删除。

Flume使用事务的办法来保证Events的可靠传递。Source和Sink分别被封装在事务中，事务由保存Event的存储或者Channel提供。这就保证了Event在数据流的点对点传输中是可靠的。在多跳的数据流中，上一跳的sink和下一跳的source均运行事务来保证数据被安全地存储到下一跳的channel中。

# Flume安装

Flume下载页面：http://flume.apache.org/download.html

![image-20200323142300139](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706048.png)

将[ apache-flume-1.9.0-bin.tar.gz](http://www.apache.org/dyn/closer.lua/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz)下载到CentOS系统中，对其解压

```shell
# 解压命令
tar xzf apache-flume-1.9.0-bin.tar.gz
# 进入到apache-flume-1.9.0-bin 目录
cd apache-flume-1.9.0-bin
# Flume使用需要依赖JDK1.8以上环境，确保已安装
# 将Flume安装目录配置到PATH中，方便在任意目录使用
vi /etc/profile
# 添加以下内容
export JAVA_HOME=/opt/soft/jdk1.8.0_231
export JRE_HOME=/opt/soft/jdk1.8.0_231/jre
export FLUME_HOME=/opt/soft/apache-flume-1.9.0-bin
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$FLUME_HOME/bin:
# 保存成功后刷新
source /etc/profile
# 查看是否设置成功
echo $FLUME_HOME
```

# 入门使用示例

## 案例说明

使用Flume监听某个端口，使用Netcat向这个端口发送数据，Flume将接收到的数据打印到控制台。

Netcat是一款TCP/UDP测试工具，可以通过以下命令安装

```shell
yum install -y nc

# 开启一个服务端
nc -lk localhost 44444

# 新建一个会话，开启一个客户端，连接端口44444
nc localhost 44444

# 客户端发送数据，服务端就能接收数据
hello
```

## 使用组件

- [NetCat TCP Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#netcat-tcp-source)

  必须属性

  | 属性名       | 默认值 | 说明                   |
  | :----------- | :----- | :--------------------- |
  | **channels** | –      |                        |
  | **type**     | –      | `netcat`               |
  | **bind**     | –      | 绑定的主机名或者IP地址 |
  | **port**     | –      | 绑定端口               |

- [Memory Channel](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#memory-channel)

  必须属性

  | 属性名   | 默认值 | 说明     |
  | :------- | :----- | :------- |
  | **type** | –      | `memory` |

- [Logger Sink](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#logger-sink)

  必须属性

  | 属性名      | 默认值 | 说明     |
  | :---------- | :----- | :------- |
  | **channel** | –      |          |
  | **type**    | –      | `logger` |

## 添加配置文件

在apache-flume-1.9.0-bin/conf目录下添加配置文件example.conf

```properties
# example.conf: 单节点Flume配置
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source类型为NetCat,监听地址为本机，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 配置sink类型为Logger
a1.sinks.k1.type = logger

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

## 启动flume

查看Flume使用命令

```shell
flume-ng help
```

```shell
Usage: /opt/soft/apache-flume-1.9.0-bin/bin/flume-ng <command> [options]...

commands:
  help                      display this help text
  agent                     run a Flume agent
  avro-client               run an avro Flume client
  version                   show Flume version info

global options:
  --conf,-c <conf>          use configs in <conf> directory
  --classpath,-C <cp>       append to the classpath
  --dryrun,-d               do not actually start Flume, just print the command
  --plugins-path <dirs>     colon-separated list of plugins.d directories. See the
                            plugins.d section in the user guide for more details.
                            Default: $FLUME_HOME/plugins.d
  -Dproperty=value          sets a Java system property value
  -Xproperty=value          sets a Java -X option

agent options:
  --name,-n <name>          the name of this agent (required)
  --conf-file,-f <file>     specify a config file (required if -z missing)
  --zkConnString,-z <str>   specify the ZooKeeper connection to use (required if -f missing)
  --zkBasePath,-p <path>    specify the base path in ZooKeeper for agent configs
  --no-reload-conf          do not reload config file if changed
  --help,-h                 display help text

avro-client options:
  --rpcProps,-P <file>   RPC client properties file with server connection params
  --host,-H <host>       hostname to which events will be sent
  --port,-p <port>       port of the avro source
  --dirname <dir>        directory to stream to avro source
  --filename,-F <file>   text file to stream to avro source (default: std input)
  --headerFile,-R <file> File containing event headers as key/value pairs on each new line
  --help,-h              display help text

  Either --rpcProps or both --host and --port must be specified.

Note that if <conf> directory is specified, then it is always included first
in the classpath.
```

启动agent

```shell
flume-ng agent --conf conf --conf-file conf/example.conf --name a1 -Dflume.root.logger=INFO,console
```

或者

```shell
flume-ng agent -n a1 -c conf -f example.conf -Dflume.root.logger=INFO,console
```



![image-20200110174336210](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706049.png)

## 使用Netcat测试

从另一个终端启动Netcat连接到44444端口，发送一些字符串

```shell
nc localhost 44444
```

![image-20200323153910638](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706050.png)

观察agent控制台

![image-20200323154036797](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706051.png)

从这里可以看到事件由头部和字节数组组成。



在另外一台虚拟机通过Netcat连接，需要更改配置文件

```shell
# 将绑定端口配置为IP地址，绑定为localhost或者127.0.0.1在另外一台机器上无法连接
# 绑定指定ip不行，绑定0.0.0.0可以
a1.sources.r1.type = netcat
a1.sources.r1.bind = 192.168.85.132
a1.sources.r1.port = 44444
```

如果开启了防火墙，需要添加防火墙规则

```shell
firewall-cmd --zone=public --add-port=44444/tcp --permanent
firewall-cmd --reload
```



## 使用telnet测试

如果没有安装telnet，检查源

```shell
yum list | grep telnet
```

![image-20200110173530605](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706052.png)

安装telnet

```shell
yum install -y telnet.x86_64
yum install -y telnet-server.x86_64
```

启动telnet

```shell
telnet 127.0.0.1 44444
```

在控制台输入内容

![image-20200110174126132](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706053.png)



可以在flume窗口查看到消息

![image-20200110174514218](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706054.png)

# 数据持久化

使用组件

- [File Channel](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#file-channel)

属性设置

| 属性名        | 默认值                           | 说明                                                         |
| :------------ | :------------------------------- | :----------------------------------------------------------- |
| **type**      | –                                | `file`                                                       |
| checkpointDir | ~/.flume/file-channel/checkpoint | 检查点文件存放路径                                           |
| dataDirs      | ~/.flume/file-channel/data       | 日志存储路径，多个路径使用逗号分隔. 使用不同的磁盘上的多个路径能提高file channel的性能 |

添加配置文件file-channel.conf，添加一个FileChannel

```shell
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1
# 多个channel使用空格分隔
a1.channels = c1 c2

# 配置source类型为NetCat,监听地址为本机，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 配置sink类型为Logger
a1.sinks.k1.type = logger

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 配置FileChannel,checkpointDir为检查点文件存储目录，dataDirs为日志数据存储目录，
a1.channels.c2.type = file
a1.channels.c2.checkpointDir = /opt/soft/flume/checkpoint
a1.channels.c2.dataDirs = /opt/soft/flume/data

# 将source和sink绑定到channel上
# source同时绑定到c1和c2上
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
```

为了方便日志打印，可以将`-Dflume.root.logger=INFO,console`添加在conf的环境配置中，从模板复制一份配置

```shell
cp flume-env.sh.template flume-env.sh
vi flume-env.sh
# 添加JAVA_OPTS
export JAVA_OPTS="-Dflume.root.logger=INFO,console"
```

启动Flume

```shell
flume-ng agent -n a1 -c ./ -f file-channnel.conf
```



通过Netcat发送数据，，此时发送到c2的数据没有被消费，关闭Flume，修改配置文件

```shell
# 将sink绑定到c2上
a1.sinks.k1.channel = c2
```

重启Flume，可以看到会**重新消费c2的数据**

![image-20200323164045854](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706055.png)

# 日志文件监控

## 案例说明

企业中应用程序部署后会将日志写入到文件中，可以使用Flume从各个**日志文件**将日志收集到日志中心以便于查找和分析。

## 使用Exec Soucre

- [Exec Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#exec-source)

Exec Source通过指定命令监控文件的变化，加粗属性为必须设置的。

| 属性名         | 默认值      | 说明                                       |
| :------------- | :---------- | :----------------------------------------- |
| **channels**   | –           |                                            |
| **type**       | –           | `exec`                                     |
| **command**    | –           | 要执行的命令                               |
| restart        | false       | 如果执行命令挂了是否要重启                 |
| batchSize      | 20          | 同时往channel发送的最大行数                |
| batchTimeout   | 3000        | 批量发送超时时间                           |
| selector.type  | replicating | channel选择器replicating 或者 multiplexing |
| selector.*     |             | 通道选择器匹配属性                         |
| interceptors   | –           | 拦截器                                     |
| interceptors.* |             |                                            |

添加配置文件exec-log.conf

```shell
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source类型为exec,命令为 tail -F app.log
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F app.log  # 目前放在同路径下

# 配置sink类型为Logger
a1.sinks.k1.type = logger

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动Flume

```shell
flume-ng agent -n a1 -c ./ -f exec-log.conf -Dflume.root.logger=INFO,console
```

通过命令更新app.log文件

```shell
echo "abcdef">> app.log
```

可以查看agent控制台接收到了最新的日志

![image-20200323184443240](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706056.png)



存在问题：重启Flume，会重复消费

## **解决重复消费问题**

- [Taildir Source](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#taildir-source)（Apache Flume 1.7.0之后才有的）

| 属性名                         | 默认值                         | 说明                                           |
| :----------------------------- | :----------------------------- | :--------------------------------------------- |
| **channels**                   | –                              |                                                |
| **type**                       | –                              | `TAILDIR`.                                     |
| **filegroups**                 | –                              | 可以定义多个组. 每个组里包含一序列被监控的文件 |
| `**filegroups.<filegroupName>**` | –                              | 被监控文件的**绝对路径**，文件名支持正则表达式 |
| positionFile                   | ~/.flume/taildir_position.json | 记录监控文件的绝对路径、上次读取位置的json文件 |

新增dir-log.conf

```shell
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source类型为TAILDIR
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/soft/flume/position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /opt/soft/apache-flume-1.9.0-bin/conf/app.log
a1.sources.r1.filegroups.f2 = /opt/soft/apache-flume-1.9.0-bin/conf/applogs/.*log

# 配置sink类型为Logger
a1.sinks.k1.type = logger

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动Flume（消费了`app.log`中的数据）

```shell
flume-ng agent -n a1 -c ./ -f dir-log.conf
```

查看`position.json`文件，`"pos":28`

![image-20250107170236134](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706057.png)

通过命令更新app.log文件

```shell
echo "123">> app.log
```

此时查看`position.json`文件，`"pos":32`

![image-20250107171105143](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071711222.png)

停掉重启Flume，发现没有出现重复消费问题，可再次通过命令更新app.log文件验证

重复消费问题解决是因为这个position.json文件记录了被监控文件最后读取的位置



**同时Taildir Source还支持目录的监控**

在applogs目录中新建一个文件

![image-20250107171933366](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071719449.png)

Flume会话已经监控到这个文件的创建了

![image-20250107172241980](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071722017.png)

通过命令更新1.log文件

```shell
echo "12345" >> 1.log
```

查看position.json文件

![image-20250107172732288](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071727368.png)

新建2.log文件

![image-20250108134554126](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108134554126.png)

此时Flume会监控到2.log文件的创建

![image-20250108135432383](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108135432383.png)

通过命令更新2.log文件

```shell
echo "123" >> 2.log
```

此时Flume

![image-20250108135121342](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108135121342.png)

此时position.json文件

![image-20250108135815260](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108135815260.png)

# 多个agent模型

可以将多个Flume agent 程序连接在一起，其中一个agent的sink将数据发送到另一个agent的source。Avro文件格式是使用Flume通过网络发送数据的标准方法。

![image-20200324142535996](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706058.png)

从多个Web服务器收集日志，发送到一个或多个集中处理的agent，之后再发往日志存储中心：

![image-20200324151344310](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706059.png)



同样的日志发送到不同的目的地：

![image-20200324151408108](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706060.png)

将前面两个示例组合应用

第一个agent从Netcat接收数据，增加一个channel和sink，将这个sink发送到第二个agent

第二个agent在监控文件变化的同时监控从sink发送来的事件，最终输出到控制台

使用Avro Sink，必须设置以下属性

| 属性名       | 默认值 | Description            |
| :----------- | :----- | :--------------------- |
| **channel**  | –      |                        |
| **type**     | –      | `avro`                 |
| **hostname** | –      | 绑定的主机名或者IP地址 |
| **port**     | –      | 监听端口               |

使用Avro Source，必须设置以下属性

| 属性名       | 默认值 | 说明                   |
| :----------- | :----- | :--------------------- |
| **channels** | –      |                        |
| **type**     | –      | `avro`                 |
| **bind**     | –      | 绑定的主机名或者IP地址 |
| **port**     | –      | 监听端口               |

添加agent1.conf配置文件（拷贝example.conf并修改）（agent1发送agent2）

```shell
# agent.conf1:多个agent使用
# 定义agent名称为a1
# 设置3个组件的名称，配置两个sink，两个channel
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2

# 配置source类型为NetCat，监听地址为本机，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 配置sink类型为Logger
a1.sinks.k1.type = logger
# 配置sink类型为Avro
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = localhost
a1.sinks.k2.port = 55555

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送>给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# 配置第二个channel
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

# 将source和sink绑定到channel上，c1和c2通道
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
# sink绑定到c2
a1.sinks.k2.channel = c2
```

添加agent2.conf配置文件（拷贝dir-log.conf并修改）

```shell
# 定义agent名称为a1
# 设置3个组件的名称，添加r2，r1用来监控日志文件，r2用来读取agent1中的sink
a1.sources = r1 r2
a1.sinks = k1
a1.channels = c1

# 配置source类型为TAILDIR
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/soft/flume/position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /opt/soft/apache-flume-1.9.0-bin/conf/app.log
a1.sources.r1.filegroups.f2 = /opt/soft/apache-flume-1.9.0-bin/conf/applogs/.*log
# 配置source为agent1的sink
a1.sources.r2.type = avro
a1.sources.r2.bind = localhost
a1.sources.r2.port = 55555

# 配置sink类型为Logger
a1.sinks.k1.type = logger

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
# 将source r2 也绑定到 channel c1 上
a1.sources.r2.channels = c1
```

先启动agent2再启动agent1（可以先读）

```shell
flume-ng agent -n a1 -c ./ -f agent2.conf
flume-ng agent -n a1 -c ./ -f agent1.conf
```



启动agent2

![image-20250108140448412](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108140448412.png)

再启动agent1

![image-20250108140549152](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108140549152.png)

此时agent2，显示已连接指定ip和端口

![image-20250108140635411](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108140635411.png)



先往app.log中写入日志，可以在agent2看到最新数据

![image-20250108140902196](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108140902196.png)



打开Netcat连接到44444，发送数据，可以同时在agent1和agent2看到最新数据。

![image-20250108141055489](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108141055489.png)

agent1

![image-20250108141227550](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108141227550.png)

agent2

![image-20250108141138110](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108141138110.png)



# 拦截器

拦截器可以**修改或者丢弃**事件，Flume支持链式调用拦截器，拦截器定义在source中

## Host Interceptor

[Host Interceptor](https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#host-interceptor)

这个拦截器将运行agent的hostname 或者 IP地址写入到事件的headers中

| 属性名           | 默认值 | 说明                                                         |
| :--------------- | :----- | :----------------------------------------------------------- |
| **type**         | –      | `host`                                                       |
| preserveExisting | false  | 如果header已经存在host, 是否要保留 - true保留原始的，false写入当前机器 |
| useIP            | true   | true为IP地址, false为 hostname.                              |
| hostHeader       | host   | header中key的名称                                            |



添加agent3.conf（拷贝agent1.conf并修改），拦截器是添加在sources上面的（示范拦截器的配置）agent3发送到agent2

```shell
# 定义agent名称为a1
# 设置3个组件的名称，配置两个sink，两个channel
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2

# 配置source类型为NetCat，监听地址为本机，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444
# 添加拦截器
a1.sources.r1.interceptors = i1
al.sources.r1.interceptors.i1.type = host

# 配置sink类型为Logger
a1.sinks.k1.type = logger
# 配置sink类型为Avro
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = localhost
a1.sinks.k2.port = 55555 

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# 配置第二个channel
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

# 将source和sink绑定到channel上，c1和c2通道
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1 
# sink绑定到c2
a1.sinks.k2.channel = c2
```



启动agent3（启动之前先启动agent2，因为要发送到agent2）

```shell
flume-ng agent -n a1 -c ./ -f agent2.conf
flume-ng agent -n a1 -c ./ -f agent3.conf
```

可以在agent2控制台看到agent3已经连接成功

![image-20250108143000294](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108143000294.png)

打开Netcat连接到44444，发送数据

![image-20250108143655837](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108143655837.png)

agent3和agent2都能看到如下：

![image-20250108143616742](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108143616742.png)



**同时可以使用另一台虚拟机来测试**

打开另外一台虚拟机，安装好Flume（虚拟机192.168.88.131）（131连130）

添加agent_example.conf，将数据发送到虚拟机130（agent2）上（拷贝agent3.conf并修改，相当于上述agent3连agent2）

```shell
# 定义agent名称为a1
# 设置3个组件的名称，配置两个sink，两个channel
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2

# 配置source类型为NetCat，监听地址为本机(132)，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444
# 添加拦截器
a1.sources.r1.interceptors = i1
al.sources.r1.interceptors.i1.type = host

# 配置sink类型为Logger
a1.sinks.k1.type = logger
# 配置sink类型为Avro
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = 192.168.88.130
a1.sinks.k2.port = 55555 

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# 配置第二个channel
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

# 将source和sink绑定到channel上，c1和c2通道
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1 
# sink绑定到c2
a1.sinks.k2.channel = c2
```

上述配置就是从131中的Netcat收到数据，通过两个chanel，一个直接打印，一个发送给130虚拟机中（暂时不行）

这里注意需要在130上开启防火墙端口

```shell
firewall-cmd --zone=public --add-port=55555/tcp --permanent
firewall-cmd --reload
```

启动example（131上的Flume）（发送给130）

```shell
flume-ng agent -n a1 -c ./ -f agent4.conf
```

可以在130的agent2控制台看到example已经连接成功

![image-20200324153714003](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20200324153714003.png)

在132上通过Netcat发送数据，发送的是132上的example

```shell
nc localhost 44444
```

而example会发送到135上的agent2，可以在135上agent2上看到添加了headers



因此，使用Host Inteceptor可以把当前主机地址发送过来，从而区分这个日志或者事件从哪个主机发送过来的



## Timestamp Interceptor

这个拦截器将**当前时间**写入到事件的headers中

| 属性名           | 默认值    | 说明                                                         |
| :--------------- | :-------- | :----------------------------------------------------------- |
| **type**         | –         | `timestamp`                                                  |
| headerName       | timestamp | header中key的名称                                            |
| preserveExisting | false     | If the timestamp already exists, should it be preserved - true or false |

再次回到agent3中的配置，修改如下，

再添加一个拦截器（直接拷贝来一个agent4.conf）


```shell
a1.sources.r1.interceptors = i1 i2
a1.sources.r1.interceptors.i1.type = host
a1.sources.r1.interceptors.i2.type = timestamp
```

```shell
启动agent4
flume-ng agent -n a1 -c ./ -f agent4.conf
```

此时agent2控制台显示已连接

![image-20250108152342166](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108152342166.png)

打开Netcat连接到44444，发送数据

```shell
nc localhost 44444
```

![image-20250108152527595](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108152527595.png)

agent4控制台输出**timestamp**日期

![image-20250108152904610](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108152904610.png)



## Static Interceptor

运行用户对所有的事件添加**固定的header**

| 属性名           | 默认值 | 说明                                                         |
| :--------------- | :----- | :----------------------------------------------------------- |
| **type**         | –      | `static`                                                     |
| preserveExisting | true   | If configured header already exists, should it be preserved - true or false |
| key              | key    | header 中key名称                                             |
| value            | value  | header 中value值                                             |

在agent4.conf中修改即可，**自定义key value**

```shell
a1.sources.r1.interceptors = i1 i2 i3
a1.sources.r1.interceptors.i1.type = host
a1.sources.r1.interceptors.i2.type = timestamp
a1.sources.r1.interceptors.i3.type = static
a1.sources.r1.interceptors.i3.key = datacenter
a1.sources.r1.interceptors.i3.value = NEW_YORK
```

打开Netcat连接到44444，发送数据

![image-20250108154253711](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108154253711.png)

agent4控制台输出

![image-20250108154633688](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108154633688.png)



## UUID Interceptor

[UUID Interceptor](https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#uuid-interceptor)

生成唯一ID

![image-20250108160309260](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108160309260.png)

在agent4.conf中修改

```shell
a1.sources.r1.interceptors = i1 i2 i3 i4
a1.sources.r1.interceptors.i1.type = host
a1.sources.r1.interceptors.i2.type = timestamp
a1.sources.r1.interceptors.i3.type = static
a1.sources.r1.interceptors.i3.key = datacenter
a1.sources.r1.interceptors.i3.value = NEW_YORK
a1.sources.r1.interceptors.i4.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder
```

打开Netcat连接到44444，发送数据

![image-20250108160000856](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108160000856.png)

agent4控制台输出

![image-20250108160104769](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108160104769.png)



## Search and Replace Interceptor

[Search and Replace Interceptor](https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#search-and-replace-interceptor)

查找并替换

![image-20250108160243230](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108160243230.png)

```shell
a1.sources.r1.interceptors = i1 i2 i3 i4 i5
a1.sources.r1.interceptors.i1.type = host
a1.sources.r1.interceptors.i2.type = timestamp
a1.sources.r1.interceptors.i3.type = static
a1.sources.r1.interceptors.i3.key = datacenter
a1.sources.r1.interceptors.i3.value = NEW_YORK
a1.sources.r1.interceptors.i4.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder
a1.sources.r1.interceptors.i5.type = search_replace
# 支持正则表达式，查找并替换，例如手机号脱敏操作（中间几位隐藏）
a1.sources.r1.interceptors.i5.searchPattern = \\d{6}
a1.sources.r1.interceptors.i5.replaceString = ******
```

打开Netcat连接到44444，发送数据

![image-20250108160958778](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108160958778.png)

agent4控制台输出（并还是headers中，而是消息体里面做了替换）

![image-20250108161153572](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250108161153572.png)



## 自定义拦截器

新建工程，添加pom引用

```xml
<dependency>
    <groupId>org.apache.flume</groupId>
    <artifactId>flume-ng-core</artifactId>
    <version>1.9.0</version>
</dependency>
```

添加自定义拦截器

```java
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.util.List;

public class MyInterceptor implements Interceptor {
    private static final Logger logger = LoggerFactory
            .getLogger(MyInterceptor.class);
    public void initialize() {

    }

    /**
     * 拦截单个事件
     * @param event
     * @return
     */
    public Event intercept(Event event) {
        String host = event.getHeaders().get("host");
        logger.info("接收到host为："+host);
        if (host.equals("192.168.85.131")) {
            logger.info("丢弃事件");
            return null;
        }
        return event;
    }

    public List<Event> intercept(List<Event> list) {
        List<Event> newList = new ArrayList<Event>();
        for (Event event : list) {
            event = intercept(event);
            if(event!=null){
                newList.add(event);
            }
        }
        return newList;
    }

    public void close() {

    }

    public static class Builder implements Interceptor.Builder{

        public Interceptor build() {
            return new MyInterceptor();
        }

        public void configure(Context context) {

        }
    }
}
```

将项目打成jar包后复制到Flume安装目录的lib目录中

修改agent2.conf（因为agent2接收从网络发送过来的数据）

```shell
# 配置拦截器
a1.sources.r2.interceptors = i1
a1.sources.r2.interceptors.i1.type = com.itheima.flume.interceptor.MyInterceptor$Builder
```

先开启131机器上的agent4

```shell
flume-ng agent -n a1 -c ./ -f agent4.conf
```

再开启130机器上的agent2

```shell
flume-ng agent -n a1 -c ./ -f agent2.conf
```

此时再从131发送数据过来，可以看到数据被拦截了，拦截后的数据不会进入channel，即不会被sink消费

![image-20200324182058523](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706068.png)

# Channel选择器

## Replicating Channel Selector

[复制选择器](https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#replicating-channel-selector-default)，如果没有指定，这个为默认选择器

可选属性如下

| 属性名            | 默认值      | 说明          |
| :---------------- | :---------- | :------------ |
| selector.type     | replicating | `replicating` |
| selector.optional | –           | `optional`    |

使用案例：

```shell
a1.sources = r1
a1.channels = c1 c2 c3
a1.sources.r1.selector.type = replicating
a1.sources.r1.channels = c1 c2 c3
a1.sources.r1.selector.optional = c3
```

上面的配置中，c3是一个可选的channel，写入c3失败的话会被忽略，

而c1和c2没有标记为可选，如果写入c1和c2失败会导致事务的失败。

## Multiplexing Channel Selector

[多路channel选择器](https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector)，可选属性如下

| 属性名             | 默认值                | 说明           |
| :----------------- | :-------------------- | :------------- |
| selector.type      | replicating           | `multiplexing` |
| selector.header    | flume.selector.header | 键值Key        |
| selector.default   | –                     |                |
| selector.mapping.* | –                     | 路由           |

使用案例：

```shell
a1.sources = r1
a1.channels = c1 c2 c3 c4
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = state
a1.sources.r1.selector.mapping.CZ = c1 c2
a1.sources.r1.selector.mapping.US = c1 c3
a1.sources.r1.selector.default = c1 c4
```

这里通过事件的header值来判断将事件发送到哪个channel，可以配合拦截器一起使用。

```shell
# 创建multichannel目录
mkdir multichannel
cd multichannel
```

创建agent5.conf

```shell
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1 k2 k3 k4
a1.channels = c1 c2 c3 c4

# 配置source类型为NetCat,监听地址为本机，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 配置sink1类型为Logger
a1.sinks.k1.type = logger

# 配置sink2,3,4类型为Avro
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = 192.168.88.130
a1.sinks.k2.port = 4040

a1.sinks.k3.type = avro
a1.sinks.k3.hostname = 192.168.88.130
a1.sinks.k3.port = 4041

a1.sinks.k4.type = avro
a1.sinks.k4.hostname = 192.168.88.130
a1.sinks.k4.port = 4042

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

a1.channels.c3.type = memory
a1.channels.c3.capacity = 1000
a1.channels.c3.transactionCapacity = 100

a1.channels.c4.type = memory
a1.channels.c4.capacity = 1000
a1.channels.c4.transactionCapacity = 100


# 将source和sink绑定到channel上
a1.sources.r1.channels = c1 c2 c3 c4
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c2
a1.sinks.k3.channel = c3
a1.sinks.k4.channel = c4
```

添加agent6.conf

```shell
# 定义agent名称为a2
# 设置3个组件的名称
a2.sources = r1
a2.sinks = k1
a2.channels = c1

# 配置source类型为avro
a2.sources.r1.type = avro
a2.sources.r1.bind = 192.168.88.130
a2.sources.r1.port = 4040


# 配置sink类型为logger
a2.sinks.k1.type = logger

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a2.channels.c1.type = memory
a2.channels.c1.capacity = 1000
a2.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a2.sources.r1.channels = c1
a2.sinks.k1.channel = c1
```

添加agent7.conf

```shell
# 定义agent名称为a3
# 设置3个组件的名称
a3.sources = r1
a3.sinks = k1
a3.channels = c1

# 配置source类型为avro
a3.sources.r1.type = avro
a3.sources.r1.bind = 192.168.88.130
a3.sources.r1.port = 4041


# 配置sink类型为logger
a3.sinks.k1.type = logger

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a3.channels.c1.type = memory
a3.channels.c1.capacity = 1000
a3.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a3.sources.r1.channels = c1
a3.sinks.k1.channel = c1
```

添加agent8.conf

```shell
# 定义agent名称为a4
# 设置3个组件的名称
a4.sources = r1
a4.sinks = k1
a4.channels = c1

# 配置source类型为avro
a4.sources.r1.type = avro
a4.sources.r1.bind = 192.168.88.130
a4.sources.r1.port = 4042


# 配置sink类型为logger
a4.sinks.k1.type = logger

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a4.channels.c1.type = memory
a4.channels.c1.capacity = 1000
a4.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a4.sources.r1.channels = c1
a4.sinks.k1.channel = c1
```

启动agent6、agent7、agent8和agent5

```shell
flume-ng agent -n a2 -c ./ -f agent6.conf
flume-ng agent -n a3 -c ./ -f agent7.conf
flume-ng agent -n a4 -c ./ -f agent8.conf
flume-ng agent -n a1 -c ./ -f agent5.conf
```

使用Netcat往agent1发送消息，可以在agent2\3\4上看到消息



修改agent5.conf，配置通道选择器

```shell
# 配置拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = static
a1.sources.r1.interceptors.i1.key = state
a1.sources.r1.interceptors.i1.value = CZ
# 配置通道选择器，如果匹配不上就走default
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = state
a1.sources.r1.selector.mapping.CZ = c1 c2
a1.sources.r1.selector.mapping.US = c1 c3
a1.sources.r1.selector.default = c1 c4
```

运行所有agent，发送数据到agent1，可以看到只有agent2收到了数据，

修改拦截器的值为US，结果是agent3收到数据。

![image-20250109194557859](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250109194557859.png)

# Sink处理器

通过sink来消费，如果消费端出现宕机，就会造成消息的积压，可能内存就不够了或者文件很大，那如何出现消息积压问题呢？Sink处理器

可以将多个sink放入到一个组中，Sink处理器能够对一个组中所有的sink进行负载均衡，在一个sink出现临时错误时进行故障转移。

必须设置属性：

| 属性名             | 默认值    | 说明                                   |
| :----------------- | :-------- | :------------------------------------- |
| **sinks**          | –         | 组中多个sink使用空格分隔               |
| **processor.type** | `default` | `default`, `failover` 或`load_balance` |

举例：

```shell
a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2
a1.sinkgroups.g1.processor.type = failover
```

## Default Sink Processor

默认的Sink处理器只支持单个Sink

## Failover Sink Processor

**故障转移处理器**维护了一个带有优先级的sink列表，故障转移机制将失败的sink放入到一个冷却池中，如果sink成功发送了事件，将其放入到活跃池中，sink可以设置优先级，数字越高，优先级越高，如果一个sink发送事件失败，下一个有更高优先级的sink将被用来发送事件，比如，优先级100的比优先级80的先被使用，如果没有设置优先级，按配置文件中配置的顺序决定。设置属性如下：

| 属性名                  | 默认值    | 说明                   |
| :---------------------- | :-------- | :--------------------- |
| **sinks**               | –         | 组内多个sinks空格分隔  |
| **processor.type**      | `default` | `failover`             |
| **processor.priority.** | –         | 优先级                 |
| processor.maxpenalty    | 30000     | 失败sink的最大冷却时间 |

示例如下：

```shell
a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2
a1.sinkgroups.g1.processor.type = failover
a1.sinkgroups.g1.processor.priority.k1 = 5
a1.sinkgroups.g1.processor.priority.k2 = 10
a1.sinkgroups.g1.processor.maxpenalty = 10000
```



修改以上agent5.conf，直接agent9.conf

```shell
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1 k2 k3 k4
a1.channels = c1

# 配置source类型为NetCat,监听地址为本机，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 配置sink组
a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2 k3 k4
a1.sinkgroups.g1.processor.type = failover
a1.sinkgroups.g1.processor.priority.k1 = 5
a1.sinkgroups.g1.processor.priority.k2 = 10
a1.sinkgroups.g1.processor.priority.k3 = 15
a1.sinkgroups.g1.processor.priority.k4 = 20
a1.sinkgroups.g1.processor.maxpenalty = 10000

# 配置sink1类型为Logger
a1.sinks.k1.type = logger

# 配置sink2,3,4类型为Avro
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = 192.168.88.130
a1.sinks.k2.port = 4040

a1.sinks.k3.type = avro
a1.sinks.k3.hostname = 192.168.88.130
a1.sinks.k3.port = 4041

a1.sinks.k4.type = avro
a1.sinks.k4.hostname = 192.168.88.130
a1.sinks.k4.port = 4042

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c1
a1.sinks.k3.channel = c1
a1.sinks.k4.channel = c1
```

启动agent6\7\8和agent9，通过Netcat发送消息到agent9，可以看到消息一直发送给其中一个agent（agent8优先级最高），将这个agent关闭后，消息会发送到其他agent（优先级次高）。

## Load balancing Sink Processor

**负载均衡处理器**，可以通过**轮询或者随机**的方式进行负载均衡，也可以通过继承AbstractSinkSelector 自定义负载均衡，设置属性如下：

| 属性名                        | 默认值        | 说明                                        |
| :---------------------------- | :------------ | :------------------------------------------ |
| **processor.sinks**           | –             | 组内多个sinks空格分隔                       |
| **processor.type**            | `default`     | `load_balance`                              |
| processor.backoff             | false         | 是否将失败的sink加入黑名单                  |
| processor.selector            | `round_robin` | 轮询机制:`round_robin`, `random` 或者自定义 |
| processor.selector.maxTimeOut | 30000         | 黑名单有效时间（单位毫秒）                  |

示例如下：

```shell
a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2
a1.sinkgroups.g1.processor.type = load_balance
a1.sinkgroups.g1.processor.backoff = true
a1.sinkgroups.g1.processor.selector = round_robin
```

修改上面的agent9.conf，直接agent10.conf，将所有的sink都绑定到c1上

```shell
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1 k2 k3 k4
a1.channels = c1

# 配置source类型为NetCat,监听地址为本机，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 配置sink组
a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2 k3 k4
a1.sinkgroups.g1.processor.type = load_balance
a1.sinkgroups.g1.processor.backoff = true
a1.sinkgroups.g1.processor.selector = round_robin

# 配置sink1类型为Logger
a1.sinks.k1.type = logger

# 配置sink2,3,4类型为Avro
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = 192.168.88.130
a1.sinks.k2.port = 4040

a1.sinks.k3.type = avro
a1.sinks.k3.hostname = 192.168.88.130
a1.sinks.k3.port = 4041

a1.sinks.k4.type = avro
a1.sinks.k4.hostname = 192.168.88.130
a1.sinks.k4.port = 4042

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c1
a1.sinks.k3.channel = c1
a1.sinks.k4.channel = c1
```

也可以修改成随机·`random`，试下效果

# 使用Flume导入数据到HDFS

在这里安装了hadoop集群，用户名为hadoop

数据导出到HDFS需要使用HDFS Sink，需要配置属性如下：

| 属性名        | 默认值       | 说明                                                         |
| :------------ | :----------- | :----------------------------------------------------------- |
| **channel**   | –            |                                                              |
| **type**      | –            | `hdfs`                                                       |
| **hdfs.path** | –            | HDFS 文件路径 (例如 hdfs://namenode/flume/webdata/)          |
| hdfs.fileType | SequenceFile | 文件格式:  `SequenceFile`, `DataStream` or `CompressedStream` (1)DataStream 不会压缩输出文件且不用设置 codeC (2)CompressedStream 需要设置 hdfs.codeC |
| hdfs.codeC    |              | 压缩格式 : gzip, bzip2, lzo, lzop, snappy                    |

注：使用HDFS Sink需要用到Hadoop的多个包，可以在装有Hadoop的主机上运行Flume，

如果是单独部署的Flume，可以通过多个Agent的形式将单独部署的Flume Agent 日志数据发送到装有Hadoop的Flume Agent上。

创建hdfs.conf 

```properties
# agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source类型为NetCat,监听地址为本机，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 配置sink类型为hdfs
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://node:8020/user/flume/logs
a1.sinks.k1.hdfs.fileType = DataStream

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动flume

```shell
flume-ng agent -n a1 -c ./ -f hdfs.conf

bin/flume-ng agent --conf conf/ --conf-file conf/hdfs.conf -Dfile.root.logger=debug,info,console --name hdfs
```

注：如果出现`com.google.common.base.Preconditions.checkArgument` 查看下`flume/lib`目录下

的`guava.jar`版本是否与`hadoop/share/hadoop/common/lib`中的版本是否一致，不一致拷贝新版

重新运行

![image-20250110105658916](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250110105658916.png)

从另一个终端启动Netcat连接到44444端口，发送一些字符串

![image-20250110105949785](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250110105949785.png)

![image-20200112212041558](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706069.png)

在后台查看

```shell
hadoop fs -cat /user/flume/messages/flume-.1578835130630
```

![image-20200112212150353](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706070.png)

# 使用多个Agent导出数据到HDFS

130->131

agent11.conf（130机器上）

```shell
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source类型为NetCat，监听地址为本机，端口为44444
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 配置sink类型为Avro
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = 192.168.88.131
a1.sinks.k1.port = 4040

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

multi-agent.conf（131机器上）

```shell
# agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source类型为avro
a1.sources.r1.type = avro
a1.sources.r1.bind = 192.168.88.131
a1.sources.r1.port = 4040

# 配置sink类型为hdfs
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://node:8020/user/flume/logs
a1.sinks.k1.hdfs.fileType = DataStream

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送
给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动agent

```shell
flume-ng agent -n a1 -c ./ -f multi-agent.conf

flume-ng agent -n a1 -c ./ -f agent11.conf
```

从130机器另一个终端启动Netcat连接到44444端口，发送一些字符串

![image-20250113114845680](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250113114845680.png)

在131机器上可以看到

![image-20250113114959939](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250113114959939.png)

浏览器访问http://node:9870/，点击Utilities中的Browse the file system

![image-20250113115334039](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250113115334039.png)

`hadoop`中查看

![image-20250113130802139](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250113130802139.png)

# Flume SDK



## 自定义Source

> 想去哪里获取数据就可以自定义

[自定义Source示例](https://flume.apache.org/releases/content/1.11.0/FlumeDeveloperGuide.html#source)

```java
public class MySource extends AbstractSource implements Configurable, PollableSource {
  private String myProp;

  @Override
  public void configure(Context context) {
    String myProp = context.getString("myProp", "defaultValue");

    // Process the myProp value (e.g. validation, convert to another type, ...)

    // Store myProp for later retrieval by process() method
    this.myProp = myProp;
  }

  @Override
  public void start() {
    // Initialize the connection to the external client
  }

  @Override
  public void stop () {
    // Disconnect from external client and do any additional cleanup
    // (e.g. releasing resources or nulling-out field values) ..
  }

  @Override
  public Status process() throws EventDeliveryException {
    Status status = null;

    try {
      // This try clause includes whatever Channel/Event operations you want to do

      // Receive new data
      Event e = getSomeData();

      // Store the Event into this Source's associated Channel(s)
      getChannelProcessor().processEvent(e);

      status = Status.READY;
    } catch (Throwable t) {
      // Log exception, handle individual exceptions as needed

      status = Status.BACKOFF;

      // re-throw all Errors
      if (t instanceof Error) {
        throw (Error)t;
      }
    } finally {
      txn.close();
    }
    return status;
  }
}
```

添加MySource

```java
package org.example.flumedemo.source;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.EventDeliveryException;
import org.apache.flume.PollableSource;
import org.apache.flume.conf.Configurable;
import org.apache.flume.event.SimpleEvent;
import org.apache.flume.source.AbstractSource;

/**
 * 自定义Source
 */
public class MySource extends AbstractSource implements Configurable, PollableSource {

    // 处理数据
    @Override
    public Status process() throws EventDeliveryException {

        Status status = null;
        try {
            // 自己模拟数据来发送
            for (int i = 0; i < 10; i++) {
                Event event = new SimpleEvent();
                event.setBody(("data:" + i).getBytes());
                // 将数据存储到与Source关联的Channel中
                getChannelProcessor().processEvent(event);
                // 数据准备消费
                status = Status.READY;
            }
            // 休眠5秒
            Thread.sleep(5000);
        } catch (Exception e) {
            // 打印日志
            e.printStackTrace();
            status = Status.BACKOFF;
        }

        return status;
    }

    @Override
    public long getBackOffSleepIncrement() {
        return 0;
    }

    @Override
    public long getMaxBackOffSleepInterval() {
        return 0;
    }

    @Override
    public void configure(Context context) {

    }
}

```

添加完自定义Source后，然后package打包一下，复制上传到/opt/soft/flume/lib/下



添加mySourceAgent.conf

```shell
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source类型为mysource
a1.sources.r1.type = org.example.flumedemo.source.MySource

# 配置sink类型为Logger
a1.sinks.k1.type = logger

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动Flume

```shell
flume-ng agent -n a1 -c ./ -f mySourceAgent.conf
```

输出结果

![image-20250113140244503](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250113140244503.png)

## 自定义Sink

[自定义Sinkf示例](https://flume.apache.org/releases/content/1.11.0/FlumeDeveloperGuide.html#sink)

```java
public class MySink extends AbstractSink implements Configurable {
  private String myProp;

  @Override
  public void configure(Context context) {
    String myProp = context.getString("myProp", "defaultValue");

    // Process the myProp value (e.g. validation)

    // Store myProp for later retrieval by process() method
    this.myProp = myProp;
  }

  @Override
  public void start() {
    // Initialize the connection to the external repository (e.g. HDFS) that
    // this Sink will forward Events to ..
  }

  @Override
  public void stop () {
    // Disconnect from the external respository and do any
    // additional cleanup (e.g. releasing resources or nulling-out
    // field values) ..
  }

  @Override
  public Status process() throws EventDeliveryException {
    Status status = null;

    // Start transaction
    Channel ch = getChannel();
    Transaction txn = ch.getTransaction();
    txn.begin();
    try {
      // This try clause includes whatever Channel operations you want to do

      Event event = ch.take();

      // Send the Event to the external repository.
      // storeSomeData(e);

      txn.commit();
      status = Status.READY;
    } catch (Throwable t) {
      txn.rollback();

      // Log exception, handle individual exceptions as needed

      status = Status.BACKOFF;

      // re-throw all Errors
      if (t instanceof Error) {
        throw (Error)t;
      }
    }
    return status;
  }
}
```

添加MySink，可以参考LoggerSink

```java
package org.example.flumedemo.sink;

import org.apache.flume.*;
import org.apache.flume.conf.Configurable;
import org.apache.flume.sink.AbstractSink;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MySink extends AbstractSink implements Configurable {
    private static final Logger LOGGER = LoggerFactory.getLogger(MySink.class);

    // 处理数据
    @Override
    public Status process() throws EventDeliveryException {
        Status status = null;
        // 获取跟Sink绑定的Channel
        Channel ch = getChannel();
        // 获取事务
        Transaction transaction = ch.getTransaction();
        try {
            // 开启事务
            transaction.begin();
            // 从channel中接收数据
            Event event = ch.take();
            if (event == null) {
                status = Status.BACKOFF;
            } else {
                // 可以将数据发送到外部存储
                // storeSomeData(e);
                // 模拟实现LoggerSink的功能
                LOGGER.info(new String(event.getBody()));
                status = Status.READY;
            }
            //提交事务
            transaction.commit();

        } catch (Exception e) {
            // 事务回滚
            transaction.rollback();
            // 打印异常
            LOGGER.error(e.getMessage());
            // 返回失败状态
            status = Status.BACKOFF;
        } finally {
            // 关闭事务
            transaction.close();
        }
        return status;
    }

    @Override
    public void configure(Context context) {

    }
}
```

重新package打包，上传到opt/soft/flume/lib/



修改上面的mySourceAgent.conf

```shell
# 定义agent名称为a1
# 设置3个组件的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source类型为mysource
a1.sources.r1.type = com.itheima.flume.source.MySource

# 配置sink类型为MySink
a1.sinks.k1.type = com.itheima.flume.sink.MySink

# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 将source和sink绑定到channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动Flume

```shell
flume-ng agent -n a1 -c ./ -f mySourceAgent.conf
```

输出结果（相当于自己实现logger）

![image-20250113143840898](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/image-20250113143840898.png)

# Flume监控

Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。

Ganglia的核心包含**gmond（监控守护进程）、gmetad（元数据守护进程）以及一个Web前端。**

主要是用来**监控系统性能**，如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。

## Ganglia安装

**中心节点的安装 **

- epel包的安装：yum install -y epel-release(解决不能yum安装某些安装包的问题)
- gmetad的安装：yum install -y ganglia-gmetad
- gmond的安装：yum install -y ganglia-gmond（监控守护进程会使用到下面的工具rrdtool）
- rrdtool的安装：yum install -y rrdtool
- httpd服务器的安装：yum install -y httpd（Web前端需要部署在httpd上）
- ganglia-web及php安装：yum install -y ganglia-web php（ganglia前端是php写的）

**被监测节点的安装**

- epel包的安装：yum install -y epel-release(解决不能yum安装某些安装包的问题)
- gmond的安装：yum install -y gmond(提示找不到，感觉应该换成上面那个yum install -y ganglia-gmond)

## Ganglia配置

**中心节点的配置** 
安装目录说明

- ganglia配置文件目录：/etc/ganglia
- rrd数据库存放目录：/var/lib/ganglia/rrds
- ganglia-web安装目录：/usr/share/ganglia
- ganglia-web配置目录：/etc/httpd/conf.d/ganglia.conf

相关配置文件修改 
将ganglia-web的站点目录连接到httpd主站点目录

```shell
# 把 /usr/share/ganglia 链接到 /var/www/html/ 下，默认以ganglia命令
$  ln -s /usr/share/ganglia /var/www/html
```

修改httpd主站点目录下ganglia站点目录的访问权限 
将ganglia站点目录访问权限改为apache:apache，否则会报错

```shell
$  chown -R apache:apache /var/www/html/ganglia
$  chmod -R 755 /var/www/html/ganglia
```

修改rrd数据库存放目录访问权限 
将rrd数据库存放目录访问权限改为nobody:nobody,否则会报错

```shell
$  chown -R nobody:nobody /var/lib/ganglia/rrds
```

修改ganglia-web的访问权限： 
修改/etc/httpd/conf.d/ganglia.conf

```shell
Alias /ganglia /usr/share/ganglia
<Location /ganglia> 
 Require all granted
 #Require ip 10.1.2.3
 #Require host example.org
</Location>
```

修改dwoo下面的权限

```shell
chmod 777 /var/lib/ganglia/dwoo/compiled
chmod 777 /var/lib/ganglia/dwoo/cache
```

配置/etc/ganglia/gmetad.conf

```shell
data_source  "my cluster" 192.168.88.130:8649(注意是所有节点都加上，如master:8649 slave0x:8649)
 
setuid_username nobody
```

配置/etc/ganglia/gmond.conf

```shell
cluster { 
  name = "my cluster"
  ... 
} 
udp_send_channel { 
  # the host who gather this cluster's monitoring data and send these data   to gmetad node
  #注释掉多播模式的,以下出现这个都要注释掉
 #mcast_join = 239.2.11.71
 #添加单播模式的
 host = 192.168.88.130
 port = 8649 
} 
udp_recv_channel { 
  # 0.0.0.0
  bind = 0.0.0.0
  port = 8649 
} 
tcp_accept_channel { 
  port = 8649 
}
```

**被监测节点的配置**
配置/etc/ganglia/gmond.conf

```shell
cluster { 
  name = "hadoop cluster"
  ... 
} 
udp_send_channel { 
  # the host who gather this cluster's monitoring data and send these data   to gmetad node
 host = 192.168.26.139  
 port = 8649 
} 
udp_recv_channel { 
  port = 8649 
} 
tcp_accept_channel { 
  port = 8649 
}
```

## Ganglia启动

**中心节点的启动** 
start httpd, gmetad, gmond

```shell
$ systemctl start httpd.service
$ systemctl start gmetad.service
$ systemctl start gmond.service
$ systemctl enable httpd.service
$ systemctl enable gmetad.service
$ systemctl enable gmond.service


# 也可以使用
sevice httpd start
servie httpd status

sevice gmetad start
servie gmetad status

sevice gmond start
servie gmond status
```

**被监测节点的启动** 
start gmond

```shell
$ systemctl start gmond.service
$ systemctl enable gmond.service
```

**关闭selinux**

vi /etc/selinux/config，把SELINUX=enforcing改成SELINUX=disable；该方法需要重启机器。 
可以使用命令setenforce 0来关闭selinux而不需要重启，刷新页面，即可访问；不过此法只是权宜之计，如果想永久修改selinux设置，还是要使用第一种方法

**开启防火墙**（外网访问需要开启端口，默认80。目前机器防火墙是关闭的）

```shell
firewall-cmd --zone=public --add-port=80/tcp --permanent
firewall-cmd --reload
```

**访问网页** 
浏览器访问 {namenode的ip}/ganglia即可 

![image-20200326103901392](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501071706071.png)

## 添加Flume监控

修改flume-env.sh配置，添加虚拟机选项

```shell
JAVA_OPTS="-Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.88.130:8649 -Xms100m -Xmx200m"
```

![image-20250117151019808](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501171510906.png)

启动flume

```shell
flume-ng agent -n a1 -c ./ -f mySourceAgent.conf -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.88.130:8649
```

查看ganglia页面

![image-20250117150708576](https://raw.githubusercontent.com/onetioner/img/main/PicGo1/202501171507683.png)



| 字段（图表名称）      | 字段含义                                                     |
| --------------------- | ------------------------------------------------------------ |
| EventPutAttemptCount  | source尝试写入channel的事件总数量                            |
| EventPutSuccessCount  | 成功写入channel且提交的事件总数量                            |
| EventTakeAttemptCount | sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据。 |
| EventTakeSuccessCount | sink成功读取的事件的总数量                                   |
| StartTime             | channel启动的时间（毫秒）                                    |
| StopTime              | channel停止的时间（毫秒）                                    |
| ChannelSize           | 目前channel中事件的总数量                                    |
| ChannelFillPercentage | channel占用百分比                                            |
| ChannelCapacity       | channel的容量                                                |

目前mySourceAgent.conf配置文件中

```shell
# channel的容量为1000
a1.channels.c1.capacity = 1000
```

每张图片可点击查看具体情况